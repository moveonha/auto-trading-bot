# ğŸ¤– ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œìŠ¤í…œ
# ì•”í˜¸í™”í AI íŠ¸ë ˆì´ë”© ë´‡ ìµœì í™”

import pandas as pd
import numpy as np
import pandas_ta as ta
from sklearn.model_selection import train_test_split, StratifiedKFold, TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
import optuna
import joblib
import json
import warnings
from datetime import datetime
import os
from supabase import create_client

warnings.filterwarnings('ignore')

class AutoTradingBot:
    def __init__(self, supabase_url, supabase_key):
        """ìë™ íŠ¸ë ˆì´ë”© ë´‡ ì´ˆê¸°í™”"""
        self.supabase = create_client(supabase_url, supabase_key)
        self.models = {}
        self.results = {}
        self.best_params = {}

    def collect_data(self, symbol='ADAUSDT', timeframe='1m', limit=100000):
        """ë°ì´í„° ìˆ˜ì§‘"""
        print(f'ğŸ”„ {symbol} {timeframe} ë°ì´í„° ìˆ˜ì§‘ ì¤‘...')

        response = self.supabase.table('crypto_ohlcv').select('*').eq('symbol', symbol.upper()).eq('timeframe', timeframe).order('timestamp', desc=True).limit(limit).execute()

        if not response.data:
            raise ValueError(f'ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {symbol} {timeframe}')

        df = pd.DataFrame(response.data)
        df['datetime'] = pd.to_datetime(df['timestamp'], unit='ms')
        df = df.sort_values('datetime').reset_index(drop=True)

        print(f'âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(df):,}ê°œ')
        print(f'ğŸ“… ê¸°ê°„: {df["datetime"].min()} ~ {df["datetime"].max()}')

        return df

    def calculate_features(self, df):
        """ê³ ê¸‰ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        print('ğŸ§® ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì¤‘...')

        # ê¸°ë³¸ ì´ë™í‰ê· 
        for period in [5, 10, 20, 50]:
            df[f'sma_{period}'] = ta.sma(df['close'], length=period)
            df[f'ema_{period}'] = ta.ema(df['close'], length=period)

        # MACD (ë‹¤ì–‘í•œ ì„¤ì •)
        macd_fast = ta.macd(df['close'], fast=6, slow=13, signal=4)
        macd_slow = ta.macd(df['close'], fast=12, slow=26, signal=9)

        df['macd_fast'] = macd_fast['MACD_6_13_4']
        df['macd_signal_fast'] = macd_fast['MACDs_6_13_4']
        df['macd_hist_fast'] = macd_fast['MACDh_6_13_4']

        df['macd_slow'] = macd_slow['MACD_12_26_9']
        df['macd_signal_slow'] = macd_slow['MACDs_12_26_9']
        df['macd_hist_slow'] = macd_slow['MACDh_12_26_9']

        # RSI (ë‹¤ì–‘í•œ ê¸°ê°„)
        for period in [9, 14, 21]:
            df[f'rsi_{period}'] = ta.rsi(df['close'], length=period)

        # ë³¼ë¦°ì € ë°´ë“œ
        bb_short = ta.bbands(df['close'], length=10, std=2)
        bb_long = ta.bbands(df['close'], length=20, std=2)

        df['bb_upper_short'] = bb_short['BBU_10_2.0']
        df['bb_lower_short'] = bb_short['BBL_10_2.0']
        df['bb_width_short'] = (df['bb_upper_short'] - df['bb_lower_short']) / df['close']
        df['bb_position_short'] = (df['close'] - df['bb_lower_short']) / (df['bb_upper_short'] - df['bb_lower_short'])

        df['bb_upper_long'] = bb_long['BBU_20_2.0']
        df['bb_lower_long'] = bb_long['BBL_20_2.0']
        df['bb_width_long'] = (df['bb_upper_long'] - df['bb_lower_long']) / df['close']
        df['bb_position_long'] = (df['close'] - df['bb_lower_long']) / (df['bb_upper_long'] - df['bb_lower_long'])

        # ìŠ¤í† ìºìŠ¤í‹±
        stoch = ta.stoch(df['high'], df['low'], df['close'])
        df['stoch_k'] = stoch['STOCHk_14_3_3']
        df['stoch_d'] = stoch['STOCHd_14_3_3']

        # ê°€ê²© ë³€í™”ìœ¨
        for period in [1, 3, 5, 10, 20]:
            df[f'return_{period}'] = df['close'].pct_change(period)

        # ê±°ë˜ëŸ‰ ì§€í‘œ
        df['volume_sma'] = ta.sma(df['volume'], length=20)
        df['volume_ratio'] = df['volume'] / df['volume_sma']
        df['volume_ema'] = ta.ema(df['volume'], length=20)

        # ì‹œê°„ íŠ¹ì„±
        df['hour'] = df['datetime'].dt.hour
        df['day_of_week'] = df['datetime'].dt.dayofweek
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        df['is_asia_session'] = ((df['hour'] >= 0) & (df['hour'] < 8)).astype(int)
        df['is_london_session'] = ((df['hour'] >= 8) & (df['hour'] < 16)).astype(int)
        df['is_ny_session'] = ((df['hour'] >= 13) & (df['hour'] < 21)).astype(int)

        # ì¶”ê°€ ì§€í‘œ
        df['atr'] = ta.atr(df['high'], df['low'], df['close'])
        df['adx'] = ta.adx(df['high'], df['low'], df['close'])

        print('âœ… ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ')
        return df

    def create_target(self, df, lookforward=5, threshold=0.002):
        """ëª©í‘œ ë³€ìˆ˜ ìƒì„± (ì´ì§„ ë¶„ë¥˜)"""
        print(f'ğŸ¯ ëª©í‘œ ë³€ìˆ˜ ìƒì„± (lookforward={lookforward}, threshold={threshold})')

        # ë¯¸ë˜ ìˆ˜ìµë¥  ê³„ì‚°
        df['future_return'] = df['close'].shift(-lookforward) / df['close'] - 1

        # ì´ì§„ ë¶„ë¥˜: ìƒìŠ¹(1) vs í•˜ë½(0)
        df['target'] = np.where(df['future_return'] > threshold, 1, 0)

        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        class_dist = df['target'].value_counts()
        print(f'ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬: {dict(class_dist)}')
        print(f'ğŸ“ˆ ìƒìŠ¹ ë¹„ìœ¨: {df["target"].mean()*100:.1f}%')

        return df

    def prepare_features(self, df):
        """íŠ¹ì„± ì¤€ë¹„ ë° ì „ì²˜ë¦¬"""
        print('ğŸ”§ íŠ¹ì„± ì¤€ë¹„ ì¤‘...')

        # ì‚¬ìš©í•  íŠ¹ì„±ë“¤
        feature_columns = [
            # ì´ë™í‰ê· 
            'sma_5', 'sma_10', 'sma_20', 'sma_50',
            'ema_5', 'ema_10', 'ema_20', 'ema_50',

            # MACD
            'macd_fast', 'macd_signal_fast', 'macd_hist_fast',
            'macd_slow', 'macd_signal_slow', 'macd_hist_slow',

            # RSI
            'rsi_9', 'rsi_14', 'rsi_21',

            # ë³¼ë¦°ì € ë°´ë“œ
            'bb_upper_short', 'bb_lower_short', 'bb_width_short', 'bb_position_short',
            'bb_upper_long', 'bb_lower_long', 'bb_width_long', 'bb_position_long',

            # ìŠ¤í† ìºìŠ¤í‹±
            'stoch_k', 'stoch_d',

            # ìˆ˜ìµë¥ 
            'return_1', 'return_3', 'return_5', 'return_10', 'return_20',

            # ê±°ë˜ëŸ‰
            'volume_sma', 'volume_ratio', 'volume_ema',

            # ì‹œê°„ íŠ¹ì„±
            'hour', 'day_of_week', 'is_weekend',
            'is_asia_session', 'is_london_session', 'is_ny_session',

            # ì¶”ê°€ ì§€í‘œ
            'atr', 'adx'
        ]

        # ê²°ì¸¡ê°’ ì œê±°
        all_columns = feature_columns + ['target']
        df_clean = df[all_columns].dropna()

        X = df_clean[feature_columns]
        y = df_clean['target']

        print(f'âœ… íŠ¹ì„± ì¤€ë¹„ ì™„ë£Œ: {X.shape}')
        return X, y, feature_columns

    def objective_lightgbm(self, trial, X, y):
        """LightGBM í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª©ì  í•¨ìˆ˜"""
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì •ì˜
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 2000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'num_leaves': trial.suggest_int('num_leaves', 10, 100),
            'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'random_state': 42,
            'n_jobs': -1,
            'verbose': -1
        }

        # êµì°¨ ê²€ì¦
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        scores = []

        for train_idx, val_idx in cv.split(X, y):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            model = lgb.LGBMClassifier(**params)
            model.fit(X_train, y_train)

            y_pred = model.predict(X_val)
            score = f1_score(y_val, y_pred, average='weighted')
            scores.append(score)

        return np.mean(scores)

    def objective_xgboost(self, trial, X, y):
        """XGBoost í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ëª©ì  í•¨ìˆ˜"""
        params = {
            'n_estimators': trial.suggest_int('n_estimators', 100, 2000),
            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
            'max_depth': trial.suggest_int('max_depth', 3, 12),
            'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
            'subsample': trial.suggest_float('subsample', 0.6, 1.0),
            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
            'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
            'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
            'random_state': 42,
            'n_jobs': -1
        }

        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        scores = []

        for train_idx, val_idx in cv.split(X, y):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            model = xgb.XGBClassifier(**params)
            model.fit(X_train, y_train)

            y_pred = model.predict(X_val)
            score = f1_score(y_val, y_pred, average='weighted')
            scores.append(score)

        return np.mean(scores)

    def optimize_hyperparameters(self, X, y, model_type='lightgbm', n_trials=100):
        """í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”"""
        print(f'ğŸ” {model_type.upper()} í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œì‘ (n_trials={n_trials})')

        if model_type == 'lightgbm':
            objective = lambda trial: self.objective_lightgbm(trial, X, y)
        elif model_type == 'xgboost':
            objective = lambda trial: self.objective_xgboost(trial, X, y)
        else:
            raise ValueError(f'ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}')

        study = optuna.create_study(direction='maximize')
        study.optimize(objective, n_trials=n_trials)

        print(f'âœ… ìµœì í™” ì™„ë£Œ')
        print(f'ğŸ† ìµœê³  ì ìˆ˜: {study.best_value:.4f}')
        print(f'ğŸ”§ ìµœì  íŒŒë¼ë¯¸í„°: {study.best_params}')

        return study.best_params, study.best_value

    def train_final_model(self, X, y, best_params, model_type='lightgbm'):
        """ìµœì  íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ëª¨ë¸ í›ˆë ¨"""
        print(f'ğŸš€ ìµœì¢… {model_type.upper()} ëª¨ë¸ í›ˆë ¨ ì¤‘...')

        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # ëª¨ë¸ ìƒì„± ë° í›ˆë ¨
        if model_type == 'lightgbm':
            model = lgb.LGBMClassifier(**best_params)
        elif model_type == 'xgboost':
            model = xgb.XGBClassifier(**best_params)
        else:
            raise ValueError(f'ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_type}')

        model.fit(X_train, y_train)

        # ì˜ˆì¸¡ ë° í‰ê°€
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, average='weighted'),
            'recall': recall_score(y_test, y_pred, average='weighted'),
            'f1': f1_score(y_test, y_pred, average='weighted'),
            'roc_auc': roc_auc_score(y_test, y_pred_proba)
        }

        print('ğŸ“Š ìµœì¢… ëª¨ë¸ ì„±ëŠ¥:')
        for metric, value in metrics.items():
            print(f'  {metric}: {value:.4f}')

        return model, metrics, (X_test, y_test, y_pred, y_pred_proba)

    def save_model(self, model, metrics, best_params, feature_columns, symbol, timeframe, model_type):
        """ëª¨ë¸ ë° ê²°ê³¼ ì €ì¥"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        model_name = f'{symbol}_{timeframe}_{model_type}_{timestamp}'

        # ëª¨ë¸ ì €ì¥
        model_path = f'models/{model_name}.pkl'
        os.makedirs('models', exist_ok=True)
        joblib.dump(model, model_path)

        # ê²°ê³¼ ì €ì¥
        results = {
            'model_name': model_name,
            'symbol': symbol,
            'timeframe': timeframe,
            'model_type': model_type,
            'timestamp': timestamp,
            'metrics': metrics,
            'best_params': best_params,
            'feature_columns': feature_columns
        }

        results_path = f'results/{model_name}_results.json'
        os.makedirs('results', exist_ok=True)

        with open(results_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

        print(f'ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}')
        print(f'ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {results_path}')

        return model_name

    def run_optimization(self, symbol='ADAUSDT', timeframe='1m', model_type='lightgbm', n_trials=100):
        """ì „ì²´ ìµœì í™” í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print(f'ğŸš€ {symbol} {timeframe} {model_type.upper()} ìµœì í™” ì‹œì‘')
        print('=' * 60)

        try:
            # 1. ë°ì´í„° ìˆ˜ì§‘
            df = self.collect_data(symbol, timeframe)

            # 2. íŠ¹ì„± ê³„ì‚°
            df = self.calculate_features(df)

            # 3. ëª©í‘œ ë³€ìˆ˜ ìƒì„±
            df = self.create_target(df)

            # 4. íŠ¹ì„± ì¤€ë¹„
            X, y, feature_columns = self.prepare_features(df)

            # 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”
            best_params, best_score = self.optimize_hyperparameters(X, y, model_type, n_trials)

            # 6. ìµœì¢… ëª¨ë¸ í›ˆë ¨
            model, metrics, test_results = self.train_final_model(X, y, best_params, model_type)

            # 7. ëª¨ë¸ ì €ì¥
            model_name = self.save_model(model, metrics, best_params, feature_columns, symbol, timeframe, model_type)

            print('=' * 60)
            print(f'âœ… {symbol} {timeframe} {model_type.upper()} ìµœì í™” ì™„ë£Œ!')
            print(f'ğŸ† ìµœê³  F1 ì ìˆ˜: {best_score:.4f}')
            print(f'ğŸ“Š ìµœì¢… ì •í™•ë„: {metrics["accuracy"]:.4f}')

            return model_name, metrics, best_params

        except Exception as e:
            print(f'âŒ ì˜¤ë¥˜ ë°œìƒ: {e}')
            return None, None, None

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # Supabase ì„¤ì •
    SUPABASE_URL = "https://your-project.supabase.co"
    SUPABASE_KEY = "your-anon-key"

    # ìë™ íŠ¸ë ˆì´ë”© ë´‡ ìƒì„±
    bot = AutoTradingBot(SUPABASE_URL, SUPABASE_KEY)

    # ìµœì í™” ì‹¤í–‰
    symbols = ['ADAUSDT', 'BTCUSDT', 'ETHUSDT']
    timeframes = ['1m', '5m', '15m']
    model_types = ['lightgbm', 'xgboost']

    for symbol in symbols:
        for timeframe in timeframes:
            for model_type in model_types:
                print(f'\nğŸ”„ {symbol} {timeframe} {model_type.upper()} ìµœì í™” ì¤‘...')
                model_name, metrics, best_params = bot.run_optimization(
                    symbol=symbol,
                    timeframe=timeframe,
                    model_type=model_type,
                    n_trials=50  # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
                )

                if model_name:
                    print(f'âœ… {model_name} ì™„ë£Œ!')
                else:
                    print(f'âŒ {symbol} {timeframe} {model_type} ì‹¤íŒ¨')

