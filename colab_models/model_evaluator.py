# ğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ë„êµ¬
# í›ˆë ¨ëœ AI íŠ¸ë ˆì´ë”© ëª¨ë¸ë“¤ì˜ ì„±ëŠ¥ ë¶„ì„ ë° ë¹„êµ

import pandas as pd
import numpy as np
import joblib
import json
import os
import warnings
from datetime import datetime, timedelta
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.model_selection import cross_val_score
import seaborn as sns
import matplotlib.pyplot as plt

warnings.filterwarnings('ignore')

class ModelEvaluator:
    def __init__(self, models_dir='models', results_dir='results'):
        """ëª¨ë¸ í‰ê°€ê¸° ì´ˆê¸°í™”"""
        self.models_dir = models_dir
        self.results_dir = results_dir
        self.models = {}
        self.results = {}
        self.evaluation_summary = {}

    def load_models(self):
        """ì €ì¥ëœ ëª¨ë¸ë“¤ ë¡œë“œ"""
        print('ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...')

        if not os.path.exists(self.models_dir):
            print(f'âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.models_dir}')
            return

        model_files = [f for f in os.listdir(self.models_dir) if f.endswith('.pkl')]

        for model_file in model_files:
            try:
                model_path = os.path.join(self.models_dir, model_file)
                model = joblib.load(model_path)

                # ëª¨ë¸ ì´ë¦„ì—ì„œ ì •ë³´ ì¶”ì¶œ
                model_name = model_file.replace('.pkl', '')
                parts = model_name.split('_')

                if len(parts) >= 4:
                    symbol = parts[0]
                    timeframe = parts[1]
                    model_type = parts[2]
                    timestamp = '_'.join(parts[3:])

                    key = f"{symbol}_{timeframe}_{model_type}"
                    self.models[key] = {
                        'model': model,
                        'model_name': model_name,
                        'symbol': symbol,
                        'timeframe': timeframe,
                        'model_type': model_type,
                        'timestamp': timestamp,
                        'file_path': model_path
                    }

                    print(f'âœ… {model_name} ë¡œë“œ ì™„ë£Œ')

            except Exception as e:
                print(f'âŒ {model_file} ë¡œë“œ ì‹¤íŒ¨: {e}')

        print(f'ğŸ“Š ì´ {len(self.models)}ê°œ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ')

    def load_results(self):
        """ê²°ê³¼ íŒŒì¼ë“¤ ë¡œë“œ"""
        print('ğŸ”„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì¤‘...')

        if not os.path.exists(self.results_dir):
            print(f'âŒ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {self.results_dir}')
            return

        result_files = [f for f in os.listdir(self.results_dir) if f.endswith('_results.json')]

        for result_file in result_files:
            try:
                result_path = os.path.join(self.results_dir, result_file)

                with open(result_path, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)

                model_name = result_data.get('model_name', result_file.replace('_results.json', ''))
                self.results[model_name] = result_data

                print(f'âœ… {result_file} ë¡œë“œ ì™„ë£Œ')

            except Exception as e:
                print(f'âŒ {result_file} ë¡œë“œ ì‹¤íŒ¨: {e}')

        print(f'ğŸ“Š ì´ {len(self.results)}ê°œ ê²°ê³¼ íŒŒì¼ ë¡œë“œ ì™„ë£Œ')

    def create_performance_summary(self):
        """ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” ìƒì„±"""
        print('ğŸ“Š ì„±ëŠ¥ ìš”ì•½ ìƒì„± ì¤‘...')

        summary_data = []

        for model_name, result in self.results.items():
            metrics = result.get('metrics', {})
            summary_data.append({
                'Model_Name': model_name,
                'Symbol': result.get('symbol', 'Unknown'),
                'Timeframe': result.get('timeframe', 'Unknown'),
                'Model_Type': result.get('model_type', 'Unknown'),
                'Timestamp': result.get('timestamp', 'Unknown'),
                'Accuracy': metrics.get('accuracy', 0),
                'Precision': metrics.get('precision', 0),
                'Recall': metrics.get('recall', 0),
                'F1_Score': metrics.get('f1', 0),
                'ROC_AUC': metrics.get('roc_auc', 0)
            })

        self.summary_df = pd.DataFrame(summary_data)

        if len(self.summary_df) > 0:
            print('âœ… ì„±ëŠ¥ ìš”ì•½ ìƒì„± ì™„ë£Œ')
            print(f'ğŸ“Š ì´ {len(self.summary_df)}ê°œ ëª¨ë¸ í‰ê°€')

            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤
            print('\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤:')
            print('F1 Score ê¸°ì¤€:')
            print(self.summary_df.nlargest(5, 'F1_Score')[['Model_Name', 'Symbol', 'Timeframe', 'Model_Type', 'F1_Score', 'Accuracy']])

            print('\nAccuracy ê¸°ì¤€:')
            print(self.summary_df.nlargest(5, 'Accuracy')[['Model_Name', 'Symbol', 'Timeframe', 'Model_Type', 'Accuracy', 'F1_Score']])

        return self.summary_df

    def plot_performance_comparison(self):
        """ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”"""
        if self.summary_df is None or len(self.summary_df) == 0:
            print('âŒ ì„±ëŠ¥ ìš”ì•½ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤')
            return

        print('ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ìƒì„± ì¤‘...')

        # 1. ì „ì²´ ì„±ëŠ¥ ì‚°ì ë„
        fig1 = px.scatter(
            self.summary_df,
            x='Accuracy',
            y='F1_Score',
            color='Model_Type',
            size='ROC_AUC',
            hover_data=['Symbol', 'Timeframe', 'Model_Name'],
            title='ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (Accuracy vs F1 Score)',
            labels={'Accuracy': 'ì •í™•ë„', 'F1_Score': 'F1 ì ìˆ˜', 'ROC_AUC': 'ROC AUC'}
        )
        fig1.show()

        # 2. ì‹¬ë³¼ë³„ ì„±ëŠ¥ ë¹„êµ
        fig2 = px.box(
            self.summary_df,
            x='Symbol',
            y='F1_Score',
            color='Model_Type',
            title='ì‹¬ë³¼ë³„ F1 Score ë¶„í¬'
        )
        fig2.show()

        # 3. íƒ€ì„í”„ë ˆì„ë³„ ì„±ëŠ¥ ë¹„êµ
        fig3 = px.box(
            self.summary_df,
            x='Timeframe',
            y='F1_Score',
            color='Model_Type',
            title='íƒ€ì„í”„ë ˆì„ë³„ F1 Score ë¶„í¬'
        )
        fig3.show()

        # 4. ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥ ë¹„êµ
        fig4 = px.box(
            self.summary_df,
            x='Model_Type',
            y=['Accuracy', 'F1_Score', 'ROC_AUC'],
            title='ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥ ë¹„êµ'
        )
        fig4.show()

        # 5. íˆíŠ¸ë§µ
        pivot_df = self.summary_df.pivot_table(
            values='F1_Score',
            index='Symbol',
            columns='Timeframe',
            aggfunc='mean'
        )

        fig5 = px.imshow(
            pivot_df,
            title='ì‹¬ë³¼-íƒ€ì„í”„ë ˆì„ë³„ í‰ê·  F1 Score íˆíŠ¸ë§µ',
            labels=dict(x='íƒ€ì„í”„ë ˆì„', y='ì‹¬ë³¼', color='F1 Score')
        )
        fig5.show()

        print('âœ… ì„±ëŠ¥ ë¹„êµ ì‹œê°í™” ì™„ë£Œ')

    def analyze_model_stability(self, test_data=None):
        """ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„"""
        print('ğŸ” ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„ ì¤‘...')

        stability_results = {}

        for key, model_info in self.models.items():
            model = model_info['model']
            model_name = model_info['model_name']

            try:
                # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì•ˆì •ì„± ì¸¡ì •
                if hasattr(model, 'feature_importances_'):
                    # íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
                    feature_importance = pd.DataFrame({
                        'feature': model_info.get('feature_columns', []),
                        'importance': model.feature_importances_
                    }).sort_values('importance', ascending=False)

                    stability_results[model_name] = {
                        'feature_importance': feature_importance,
                        'top_features': feature_importance.head(10).to_dict('records'),
                        'importance_std': feature_importance['importance'].std(),
                        'importance_mean': feature_importance['importance'].mean()
                    }

                    print(f'âœ… {model_name} ì•ˆì •ì„± ë¶„ì„ ì™„ë£Œ')

            except Exception as e:
                print(f'âŒ {model_name} ì•ˆì •ì„± ë¶„ì„ ì‹¤íŒ¨: {e}')

        self.stability_results = stability_results
        return stability_results

    def plot_feature_importance(self, top_n=10):
        """íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”"""
        if not hasattr(self, 'stability_results'):
            print('âŒ ì•ˆì •ì„± ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € analyze_model_stability()ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.')
            return

        print('ğŸ“Š íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ìƒì„± ì¤‘...')

        for model_name, stability_data in self.stability_results.items():
            feature_importance = stability_data['feature_importance']

            # ìƒìœ„ Nê°œ íŠ¹ì„±ë§Œ ì„ íƒ
            top_features = feature_importance.head(top_n)

            fig = px.bar(
                top_features,
                x='importance',
                y='feature',
                orientation='h',
                title=f'{model_name} - ìƒìœ„ {top_n}ê°œ ì¤‘ìš” íŠ¹ì„±',
                labels={'importance': 'ì¤‘ìš”ë„', 'feature': 'íŠ¹ì„±'}
            )
            fig.show()

        print('âœ… íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™” ì™„ë£Œ')

    def compare_model_parameters(self):
        """ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¹„êµ"""
        print('ğŸ”§ ëª¨ë¸ íŒŒë¼ë¯¸í„° ë¹„êµ ì¤‘...')

        param_comparison = []

        for model_name, result in self.results.items():
            best_params = result.get('best_params', {})

            param_data = {
                'Model_Name': model_name,
                'Symbol': result.get('symbol', 'Unknown'),
                'Timeframe': result.get('timeframe', 'Unknown'),
                'Model_Type': result.get('model_type', 'Unknown')
            }

            # íŒŒë¼ë¯¸í„° ì¶”ê°€
            for param, value in best_params.items():
                param_data[param] = value

            param_comparison.append(param_data)

        self.param_df = pd.DataFrame(param_comparison)

        if len(self.param_df) > 0:
            print('âœ… íŒŒë¼ë¯¸í„° ë¹„êµ ì™„ë£Œ')

            # ëª¨ë¸ íƒ€ì…ë³„ íŒŒë¼ë¯¸í„° ë¶„í¬
            numeric_cols = self.param_df.select_dtypes(include=[np.number]).columns
            numeric_cols = [col for col in numeric_cols if col not in ['Model_Name']]

            for col in numeric_cols:
                fig = px.box(
                    self.param_df,
                    x='Model_Type',
                    y=col,
                    title=f'{col} íŒŒë¼ë¯¸í„° ë¶„í¬ (ëª¨ë¸ íƒ€ì…ë³„)'
                )
                fig.show()

        return self.param_df

    def generate_evaluation_report(self, output_path='evaluation_report.html'):
        """ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        print('ğŸ“‹ ì¢…í•© í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...')

        # HTML ë¦¬í¬íŠ¸ ìƒì„±
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>AI íŠ¸ë ˆì´ë”© ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }}
                .section {{ margin: 20px 0; padding: 15px; border: 1px solid #ddd; border-radius: 5px; }}
                .metric {{ display: inline-block; margin: 10px; padding: 10px; background-color: #e8f4f8; border-radius: 3px; }}
                table {{ border-collapse: collapse; width: 100%; }}
                th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }}
                th {{ background-color: #f2f2f2; }}
                .highlight {{ background-color: #fff3cd; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>ğŸ¤– AI íŠ¸ë ˆì´ë”© ëª¨ë¸ í‰ê°€ ë¦¬í¬íŠ¸</h1>
                <p>ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
            </div>
        """

        # ìš”ì•½ í†µê³„
        if hasattr(self, 'summary_df') and len(self.summary_df) > 0:
            html_content += f"""
            <div class="section">
                <h2>ğŸ“Š ì „ì²´ ìš”ì•½</h2>
                <div class="metric">
                    <strong>ì´ ëª¨ë¸ ìˆ˜:</strong> {len(self.summary_df)}
                </div>
                <div class="metric">
                    <strong>í‰ê·  F1 Score:</strong> {self.summary_df['F1_Score'].mean():.4f}
                </div>
                <div class="metric">
                    <strong>í‰ê·  Accuracy:</strong> {self.summary_df['Accuracy'].mean():.4f}
                </div>
                <div class="metric">
                    <strong>ìµœê³  F1 Score:</strong> {self.summary_df['F1_Score'].max():.4f}
                </div>
            </div>

            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤
            top_models = self.summary_df.nlargest(5, 'F1_Score')
            html_content += """
            <div class="section">
                <h2>ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸ë“¤ (F1 Score ê¸°ì¤€)</h2>
                <table>
                    <tr>
                        <th>ëª¨ë¸ëª…</th>
                        <th>ì‹¬ë³¼</th>
                        <th>íƒ€ì„í”„ë ˆì„</th>
                        <th>ëª¨ë¸ íƒ€ì…</th>
                        <th>F1 Score</th>
                        <th>Accuracy</th>
                    </tr>
            """

            for _, row in top_models.iterrows():
                html_content += f"""
                    <tr>
                        <td>{row['Model_Name']}</td>
                        <td>{row['Symbol']}</td>
                        <td>{row['Timeframe']}</td>
                        <td>{row['Model_Type']}</td>
                        <td class="highlight">{row['F1_Score']:.4f}</td>
                        <td>{row['Accuracy']:.4f}</td>
                    </tr>
                """

            html_content += """
                </table>
            </div>
            """

        # ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥
        if hasattr(self, 'summary_df') and len(self.summary_df) > 0:
            model_type_stats = self.summary_df.groupby('Model_Type').agg({
                'F1_Score': ['mean', 'std', 'count'],
                'Accuracy': ['mean', 'std']
            }).round(4)

            html_content += """
            <div class="section">
                <h2>ğŸ“ˆ ëª¨ë¸ íƒ€ì…ë³„ ì„±ëŠ¥ í†µê³„</h2>
                <table>
                    <tr>
                        <th>ëª¨ë¸ íƒ€ì…</th>
                        <th>ëª¨ë¸ ìˆ˜</th>
                        <th>í‰ê·  F1 Score</th>
                        <th>F1 Score í‘œì¤€í¸ì°¨</th>
                        <th>í‰ê·  Accuracy</th>
                        <th>Accuracy í‘œì¤€í¸ì°¨</th>
                    </tr>
            """

            for model_type in model_type_stats.index:
                stats = model_type_stats.loc[model_type]
                html_content += f"""
                    <tr>
                        <td>{model_type}</td>
                        <td>{stats[('F1_Score', 'count')]}</td>
                        <td>{stats[('F1_Score', 'mean')]:.4f}</td>
                        <td>{stats[('F1_Score', 'std')]:.4f}</td>
                        <td>{stats[('Accuracy', 'mean')]:.4f}</td>
                        <td>{stats[('Accuracy', 'std')]:.4f}</td>
                    </tr>
                """

            html_content += """
                </table>
            </div>
            """

        html_content += """
        </body>
        </html>
        """

        # HTML íŒŒì¼ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)

        print(f'âœ… í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {output_path}')
        return output_path

    def run_full_evaluation(self):
        """ì „ì²´ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        print('ğŸš€ ì „ì²´ ëª¨ë¸ í‰ê°€ ì‹œì‘')
        print('=' * 60)

        # 1. ëª¨ë¸ ë° ê²°ê³¼ ë¡œë“œ
        self.load_models()
        self.load_results()

        # 2. ì„±ëŠ¥ ìš”ì•½ ìƒì„±
        self.create_performance_summary()

        # 3. ì„±ëŠ¥ ë¹„êµ ì‹œê°í™”
        self.plot_performance_comparison()

        # 4. ëª¨ë¸ ì•ˆì •ì„± ë¶„ì„
        self.analyze_model_stability()

        # 5. íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
        self.plot_feature_importance()

        # 6. íŒŒë¼ë¯¸í„° ë¹„êµ
        self.compare_model_parameters()

        # 7. í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_evaluation_report()

        print('=' * 60)
        print('âœ… ì „ì²´ ëª¨ë¸ í‰ê°€ ì™„ë£Œ!')

        return {
            'summary_df': self.summary_df,
            'stability_results': self.stability_results,
            'param_df': self.param_df
        }

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ëª¨ë¸ í‰ê°€ê¸° ìƒì„±
    evaluator = ModelEvaluator()

    # ì „ì²´ í‰ê°€ ì‹¤í–‰
    results = evaluator.run_full_evaluation()

    # ê²°ê³¼ í™•ì¸
    print('\nğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½:')
    if results['summary_df'] is not None:
        print(f"ì´ ëª¨ë¸ ìˆ˜: {len(results['summary_df'])}")
        print(f"í‰ê·  F1 Score: {results['summary_df']['F1_Score'].mean():.4f}")
        print(f"ìµœê³  F1 Score: {results['summary_df']['F1_Score'].max():.4f}")

